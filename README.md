# Knowledge Infused Learning with Explainability

#### Group Members
Tiancheng (Robert) Shi: ts3474@columbia.edu  
Anne (Yuanchen) Wei: yw3939@columbia.edu  
Varalika Mahajan: vm2695@columbia.edu  
Vrinda Bhat: vgb2113@columbia.edu  
Zhening (Colin) Zhang: zz3040@columbia.edu

#### Mentors from Accenture
Dharmesh Pathak  
Harshaprabha Shetty  
Tanusree De

#### Mentors from Columbia
Prof. Adam S. Kelleher  

## Abstract
Large Language Models (LLMs) can sometimes amplify the inherent biases and unfairness in users' inputs. Previous research
has shown the effectiveness of prompt engineering in making LLMs generate desired outputs. Our capstone project aims to
use Knowledge-Infused Learning approaches to reduce the biases and unfairness generated by Llama 2 from Meta. We proposed
the Distilled Knowledge Graph (KG) Infusion method and others, and evaluated their improvements in mitigating gender bias
in multiple Sephora cosmetic products. We presented an end-to-end pipeline of Distilled KG method, and further proposed
a framework to fine-tune LLaMA model toward this task.

#### Examples of LLaMA biased/unbiased outputs:
![overview](https://github.com/Anemonee1212/knowledge_infused_learning/assets/64716883/7e0301ae-de37-4a2f-8c19-f6aee5f3d460)

#### Distilled KG Infusion method:
![detail2](https://github.com/Anemonee1212/knowledge_infused_learning/assets/64716883/d13549a7-528d-463e-b114-4c779294ce77)

## Directories
```bash
- knowledge_infused_learning
  ├── data
  │   ├── a.csv
  │   ├── b.csv
  │   └── ...
  ├── x.py
  ├── y.py
  └── ...
```
