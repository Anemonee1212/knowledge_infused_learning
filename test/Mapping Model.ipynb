{"cells":[{"cell_type":"markdown","metadata":{"id":"iSU_eC5zXt5A"},"source":["**Takeaway** <br>\n","1. It is much easier to mapping Llama embeddings (dim = 4096) to KG embeddings (dim ~ 3), given that Llama embeddings have higher dimensions."]},{"cell_type":"markdown","metadata":{"id":"tVrjCVsoQiZ2"},"source":["# Set up"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6725,"status":"ok","timestamp":1699933904272,"user":{"displayName":"zhening zhang","userId":"01474561848345492071"},"user_tz":300},"id":"vBPWzosgQUnJ","outputId":"eedbaaca-a16d-47d7-eb94-208e0e08f6c8"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader, random_split"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# data import\n","import os\n","import pickle\n","path_llama_emb = \"/Users/yuanchenwei/Desktop/Capstone Project/triplets_llama_embeddings.pkl\"\n","path_pykeen_emb = \"/Users/yuanchenwei/Desktop/Capstone Project/pykeen_embed.pkl\"\n","\n","with open(path_llama_emb, 'rb') as f:\n","    llama_emb = pickle.load(f) # embedding from llama -> x\n","\n","with open(path_pykeen_emb, 'rb') as f:\n","    pykeen_emb = pickle.load(f) # embedding from pykeen -> y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = []"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-0.9619, -0.6189,  0.7819,  ...,  1.1022,  0.4357,  0.8974]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["llama_emb[\"Women\"].mean(dim = 1)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# set up x and y, step 1\n","x_list_pad = []\n","x_list_avg = []\n","y_list = []\n","for entity in llama_emb.keys():\n","    # x_list_pad.append(llama_emb[entity].squeeze(dim = 0))\n","    x_list_avg.append(llama_emb[entity].mean(dim = 1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set up x and y, step 1\n","x_list_pad = []\n","x_list_avg = []\n","y_list = []\n","for entity in llama_emb.keys():\n","    x_list_pad.append(llama_emb[entity].squeeze(dim = 0)) # change [1, 4, 4096] to [4, 4096]\n","    x_list_avg.append(llama_emb[entity].mean(dim = 1)) # # change [1, 4, 4096] to [1, 4096]\n","    y_list.append(pykeen_emb[entity].unsqueeze(dim = 0)) # change [32] to [1, 32]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.DataFrame({\"x_pad\": x_list_pad, \"x_avg\": x_list_avg, \"y\": y_list})\n","data_train, data_test = train_test_split(data, test_size=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def collate_pad(batch):\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set up x and y, step 2\n","x_pad_train = nn.utils.rnn.pad_sequence(data_train[\"x_pad\"], batch_first=True) # shape: [156, 15, 4096]\n","x_avg_train = torch.cat(data_train[\"x_avg\"].to_list()) # shape[156, 4096]\n","y_train = torch.cat(data_train[\"y\"].to_list()) # shape: [156, 32]\n","print(f\"x_pad: {x_pad_train.shape}\")\n","print(f\"x_avg: {x_avg_train.shape}\")\n","print(f\"y: {y_train.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set up x and y, step 2\n","x_pad_test = nn.utils.rnn.pad_sequence(data_test[\"x_pad\"], batch_first=True) # shape: [156, 15, 4096]\n","x_avg_test = torch.cat(data_test[\"x_avg\"].to_list()) # shape[156, 4096]\n","y_test = torch.cat(data_test[\"y\"].to_list()) # shape: [156, 32]\n","print(f\"x_pad: {x_pad_test.shape}\")\n","print(f\"x_avg: {x_avg_test.shape}\")\n","print(f\"y: {y_test.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set up dataloader\n","BATCH_SIZE = 16\n","data_x_pad_train = DataLoader(x_pad_train, batch_size = BATCH_SIZE, shuffle = False) # recall that BATCH_SIZE = 16\n","data_x_avg_train = DataLoader(x_avg_train, batch_size = BATCH_SIZE, shuffle = False)\n","data_y_train = DataLoader(y_train, batch_size = BATCH_SIZE, shuffle = False)\n","data_x_pad_test = DataLoader(x_pad_test, batch_size = BATCH_SIZE, shuffle = False)\n","data_x_avg_test = DataLoader(x_avg_test, batch_size = BATCH_SIZE, shuffle = False)\n","data_y_test = DataLoader(y_test, batch_size = BATCH_SIZE, shuffle = False)\n","# train_data_avg = [(xi, yi) for xi, yi in zip(x_avg, y)]\n","# trainloader_avg = DataLoader(train_data_avg, batch_size = BATCH_SIZE, shuffle = True) # recall that BATCH_SIZE = 16"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for x in data_x_pad_test:\n","    print(x)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEAURbQDQ2D-"},"outputs":[],"source":["class Mapping(nn.Module):\n","  def __init__(self, input_size, hidden_size_lstm, hidden_size_lin, output_size):\n","    super(Mapping, self).__init__()\n","    # self.hidden_size_lstm = hidden_size_lstm\n","    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size_lstm, batch_first = True)\n","    #self.fc1 = nn.Linear(input_size, hidden_size)\n","    self.fc1 = nn.Linear(hidden_size_lstm, hidden_size_lin)\n","    self.fc2 = nn.Linear(hidden_size_lin, output_size)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, x, hidden_cell = None):\n","    #print(f\"x: {x.shape}\")\n","    # x -> [Batch_size, length, D]\n","    output, (hn, _) = self.lstm(x, hidden_cell)\n","    #print(f\"hn: {hn.shape}\")\n","    # hn -> [Batch_size, length, hidden_size]\n","    #hn = nn.ReLU()(hn)\n","    \n","    x = self.fc1(hn)\n","    x = self.relu(x)\n","    x = self.fc2(x)\n","    #print(f\"output: {output.shape}\")\n","    return x\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_e0hRn2R2B3"},"outputs":[],"source":["# recall that\n","INPUT_SIZE = 4096\n","HIDDEN_SIZE_LSTM = 512\n","HIDDEN_SIZE_LIN = 128\n","OUTPUT_SIZE = 32\n","model = Mapping(INPUT_SIZE, HIDDEN_SIZE_LSTM, HIDDEN_SIZE_LIN, OUTPUT_SIZE)\n","#model = nn.Linear(INPUT_SIZE, OUTPUT_SIZE)\n","#optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","loss_fn = nn.MSELoss()\n","\n","#def loss_func(feat1, feat2):\n","#    return - nn.functional.cosine_similarity(feat1, feat2).mean()\n","\n","#loss_fn = loss_func"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":505,"status":"ok","timestamp":1699933913287,"user":{"displayName":"zhening zhang","userId":"01474561848345492071"},"user_tz":300},"id":"s5ETEjuuSGy1","outputId":"8fbe6ce9-b01e-47f8-82ad-8c3ecdae7943"},"outputs":[],"source":["# Training\n","EPOCHS = 100\n","\n","for epoch in range(EPOCHS):\n","\n","  for x_batch, y_batch in zip(data_x_pad_train, data_y_train):\n","\n","    optimizer.zero_grad()\n","\n","    pred = model(x_batch)\n","    \n","    loss = loss_fn(pred, y_batch)\n","\n","    # Get teh gradients of l with respect to theta\n","    loss.backward()\n","\n","    # Update theta\n","    optimizer.step()\n","\n","  if (epoch + 1) % 10 == 0:   \n","    print('Epoch:{} Loss: {:.4f}'.format(epoch + 1, loss))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1699933924618,"user":{"displayName":"zhening zhang","userId":"01474561848345492071"},"user_tz":300},"id":"57qK9ivXS8zQ","outputId":"90267b1f-3841-4782-d595-64a3e875ffaf"},"outputs":[],"source":["# Evaluation\n","cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","\n","with torch.no_grad():\n","  pred = model(x).squeeze(dim = 0) # shape: [156, 32]\n","  similarity = cos(pred, y)\n","  print(similarity)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1699933932911,"user":{"displayName":"zhening zhang","userId":"01474561848345492071"},"user_tz":300},"id":"7gle7OWgWm40","outputId":"eab50373-80fd-4266-8271-06f821590c3e"},"outputs":[],"source":["i = 1\n","print(y[i])\n","print(pred[i])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPAUYwF6eNATMQX+lo91MS0","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
